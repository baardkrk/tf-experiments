from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

# setup hyperparameters and such

learning_rate = 0.5
epochs = 10
batch_size = 100

# declare the training data placeholders
# input x - for 28 x 28 pixels = 784 digits
x = tf.placeholder(tf.float32, [None, 784])
# now declare the output data placeholder - 10 digits
y = tf.placeholder(tf.float32, [None, 10])

# Declare weights connecting the input to the hidden layer
W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.03), name='W1')
b1 = tf.Variable(tf.random_normal([300]), name='b1')
# weights connecting the hidden layer to the output layer:
W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.03), name='W2')
b2 = tf.Variable(tf.random_normal([10]), name='b2')

# calculate output for the hidden layer
hidden_out = tf.add(tf.matmul(x, W1), b1)  # matrix multiplication of x and W1, add b1
hidden_out = tf.nn.relu(hidden_out)   	   # applying relu on the result

# setup output layer
y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))    # using softmax activation

y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)
cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped) + 
	      					(1 - y) * tf.log(1 - y_clipped), 
						axis=1))

